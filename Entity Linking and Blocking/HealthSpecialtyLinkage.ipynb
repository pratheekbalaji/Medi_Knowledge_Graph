{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "from re import sub as re_sub\n",
    "import sys\n",
    "import json\n",
    "import rltk\n",
    "from collections import defaultdict\n",
    "\n",
    "global g_tokenizer\n",
    "g_tokenizer = rltk.CrfTokenizer()\n",
    "\n",
    "class DocRecord(rltk.Record):\n",
    "    ''' Record entry class for each of our IMDB records '''\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = ''\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def id(self):\n",
    "        return self.raw_object['DoctorURI']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def health_sp(self):\n",
    "        return self.raw_object['Doctor_Speciality']\n",
    "\n",
    "class DiseaseRecord(rltk.Record):\n",
    "    ''' Record entry class for each of our AFI records '''\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = ''\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def id(self):\n",
    "        return self.raw_object['DiseaseURI']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def health_sp_list(self):\n",
    "        return self.raw_object['HealthSpecialty']\n",
    "\n",
    "\n",
    "def create_dataset(input_file: str, rcrd_class: rltk.Record) -> rltk.Dataset:\n",
    "    ''' Create rltk dataset from a given jl file '''\n",
    "    assert Path(input_file).suffix == \".jl\"\n",
    "    return rltk.Dataset(reader=rltk.JsonLinesReader(input_file), record_class=rcrd_class, adapter=rltk.MemoryKeyValueAdapter())\n",
    "\n",
    "def remove_words(s):\n",
    "    s=s.lower()\n",
    "    omit_words=['medical','care']\n",
    "    for i in omit_words:\n",
    "        if i in s:\n",
    "            s=s.replace(i, '')\n",
    "    s=list(s.split())\n",
    "    return s\n",
    "\n",
    "def lister(s_list):\n",
    "    new_list=[]\n",
    "    for x in s_list:\n",
    "        new_x=remove_words(x)\n",
    "        new_list.append(new_x)\n",
    "    return new_list\n",
    "\n",
    "#def get_ground_truth(input_file: str, ds1: rltk.Dataset, ds2: rltk.Dataset) -> rltk.GroundTruth:\n",
    "#    ''' Read the grouth truth from the given input file '''\n",
    "#    devset_file_handle = open(input_file, \"r\")\n",
    "#    devset_data = json.load(devset_file_handle)\n",
    "#    gt = rltk.GroundTruth()\n",
    "#    for item in devset_data:\n",
    "#        if None != item['afi_movie']:\n",
    "#            r_imdb = ds1.get_record(item['imdb_movie'])\n",
    "#            r_afi  = ds2.get_record(item['afi_movie'])\n",
    "#            gt.add_positive(r_imdb.raw_object['url'], r_afi.raw_object['url'])\n",
    "#    return gt\n",
    "\n",
    "def health_similarity(r_disease,r_doc):\n",
    "    health_list=r_disease.health_sp_list\n",
    "    hp_sp_name=r_doc.health_sp\n",
    "    \n",
    "    health_list=lister(health_list)\n",
    "    health_name=hp_sp_name\n",
    "    health_name=remove_words(health_name)\n",
    "    res = []\n",
    "    \n",
    "    checker=['medicine','disease']\n",
    "    for x in health_name:\n",
    "        if x in checker:\n",
    "            continue\n",
    "        for i in health_list:\n",
    "            for j in i:\n",
    "                if j in checker:\n",
    "                    continue\n",
    "                sim= rltk.levenshtein_similarity(x,j)\n",
    "                if sim>=0.8:\n",
    "                    res.append((True,(hp_sp_name.lower(),' '.join(list(i)))))\n",
    "    return res\n",
    "    \n",
    "r_disease_file = \"/Users/sharadsharma/Documents/KG/Project/OutputFiles/WikiDiseaseData.jl\"\n",
    "r_doc_file = \"/Users/sharadsharma/Documents/KG/Project/OutputFiles/DoctorData.jl\"\n",
    "\n",
    "ds_disease = create_dataset(r_disease_file, DiseaseRecord)\n",
    "ds_doc = create_dataset(r_doc_file, DocRecord)\n",
    "\n",
    "\n",
    "\n",
    "pred_dic=defaultdict(set)\n",
    "for r_dis in ds_disease:\n",
    "    \n",
    "    for r_doc in ds_doc:\n",
    "            value=health_similarity(r_dis, r_doc)\n",
    "           \n",
    "            for val in value:\n",
    "                pred_dic[(val[1][1])].add(r_doc.id)\n",
    "    \n",
    "pred_dic = defaultdict(list, ((k, list(v)) for k, v in pred_dic.items()))\n",
    "\n",
    "\n",
    "with open('/Users/sharadsharma/Documents/KG/Project/OutputFiles/HealthSpecialtyLinkage.json','w') as op_file:\n",
    "    json.dump(pred_dic,op_file,indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
